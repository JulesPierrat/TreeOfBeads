Id,Code,Name,Description54,AM5-7,Multi-criteria evaluation,Multi-criteria evaluation is an important aspect of decision support operations - which appear in process models. Process models in the Earth sciences describe the evolution of geo(bio)physical surface properties in time - independently from remote sensing observations. Examples of such process models on various time scales are - for instance - numerical weather prediction models (NWPs) - vegetation growth models - hydrological models - oceanographic models and climate models.\r\rObservation models and process models can supplement each other to enhance the quality of the interpretation of remote sensing data and to fill gaps in time that occur when observations are not possible owing to clouds or some other cause. Interactions are possible between observation models and process models with EO data and existing geographic information (GIS and ground measurements - supplemented with decision-support systems (DSSs)).\r\rThe process model provides information to the decision-support system - which supports management actions aimed at controlling/mitigating the process - based on an multi-criteria evaluation. A good example of this is a water management system - in which one might decide to allocate water for irrigation if the observed vegetation appears to suffer from drought stress.55,AM5-8,Spatial process models,Process models in the Earth sciences describe the evolution of geo(bio)physical surface properties in time - independently from remote sensing observations. Examples of such process models on various time scales are - for instance - numerical weather prediction models (NWPs) - vegetation growth models - hydrological models - oceanographic models and climate models.\rProcess models in the geosciences usually rely on regular observations at many locations spread over a large area. Traditionally - these observations were mostly made in the field with a variety of instruments. Remote sensing techniques have tremendously increased the capability of spatial sampling and the consistency of the surface parameters measured. RS instruments are mostly sensitive to many physical properties of the surface - some of these may not belong to the set of properties that the user is interested in. Exceptions to this are the mapping of sea-surface temperature - laser altimetry and gravimetry - which are measurements of direct geophysical interest. In the majority of cases - however - there are only indirect relationships between what is observed with the instrument and the physical object properties of interest. In these cases - the use of observation models becomes an attractive option - since these models describe the relationships between all object properties relevant for the observation and the observed remote sensing data.110,CF5-3b,Events and processes,We can structure time by events (moments) or periods (intervals). When we represent intervals by a start and an end event - we can derive temporal relationships between events and periods - such as “before” - “overlap” - and “after”.\rValid time (or world time) is the time when an event really happened - or a string of events took place. Transaction time (or database time) is the time when the event was stored in the database or GIS. Note that the time at which we store something in a database is typically (much) later than when the related event took place.\r\rProcess models in the Earth sciences describe the evolution of geo(bio)physical surface properties in time - independently from remote sensing observations. Examples of such process models on various time scales are - for instance - numerical weather prediction models (NWPs) - vegetation growth models - hydrological models - oceanographic models and climate models.\r\rProcesses on the planet Earth are complex phenomena that are taking place in space and in time - i.e. in four dimensions.\r\rIn many of these processes - differences in one dimension (e.g. height above the geoid) can be disregarded - so that two spatial dimensions and the dimension time remain. Despite this simpliﬁcation - the physical description of the phenomena remains a difﬁcult task. To better understand the processes it often helps if the same geographic region is viewed repeatedly and - if possible - also from different directions and in different wavelength regions. Integration of data from a variety of sources can be a means to retrieving information about processes that would otherwise remain undetected.236,GC,Geocomputation,The term geocomputation dates back to the first international conference on the topic in 1996 held at the University of Leeds under the title “The art and science of solving complex spatial problems with computers’. The term “geocomputation” was coined to describe the use of computer-intensive methods for knowledge discovery in physical and human geography. This new area distinguishes it  from the application of statistical techniques to spatial data in the focus on “creative and experimental applications” and in “developing relevant geo-tools within the overall context of a ‘scientific’ approach.” Other authors reinforced the unique character of geocomputation as “to provide better solutions to many geographical problems by developing new - computationally dependent tools for analysis and modelling”.  Simply defined - the interdisciplinary area of ​​geocomputation was - from the beginning - closely linked to the application of computer technology and the development of tools and applications to real-world spatio-temporal problems through the combination of geographic information system techniques - spatial modelling - cellular automata - and other non-conventional data clustering and analysis techniques.\rEven though geocomputation is still seeking to define the field conceptually) - it is closely related to computational science - the use of high-computing performance - artificial intelligence - computational intelligence - grid infrastructure and parallel computing . Nevertheless - the evolution of new computing paradigms - such as edge-fog-cloud computing  along with the new forms of data create new opportunities for the geocomputation community .  \r\rWhile the underlying idea remains intact --a diverse and interdisciplinary area of research that uses geospatial data - methods and tools for applied scientific work-- - the current approach to geocomputation differs from the founders in that it focuses more attention on open science - reproducible research practices - and in a vibrant collaborative community to develop new methods - tools and applications that are integrated into multiple application domains such as economics - sociology - geodemography - health - criminology - transportation - biology - remote sensing and cities . The theoretical roots and experimental emphasis of geocomputation makes it an excellent vehicle to creatively explore in parallel the theory and practice of the use of geospatial data in a computational way to solve real-world problems.239,GC1-3,Spatio-temporal problems and applications,While geocomputation is not daily used in GIS environments and traditional GIS projects -  it is the focus of   a vibrant collaborative and research community in developing new geocomputational methods - tools and applications that are integrated into multiple application domains such as economics - sociology - geodemography - health - criminology - transportation - biology - remote sensing and cities. Open science - reproducible research practices - and strong collaboration make geocomputing an excellent vehicle for creatively exploring together the theory and practice of using geospatial data in a computational way to solve real-world problems.258,GD,Geospatial Data,Geospatial data represent measurements of the locations and attributes of phenomena at or near Earth`s surface. Information is data made meaningful in the context of a question or problem. Information is rendered from data by analytical methods. Information quality and value depends to a large extent on the quality and currency of data (though historical data are valuable for many applications). Geospatial data may have spatial - temporal - and attribute (descriptive) components - as well as associated metadata. Data may be acquired from primary or secondary data sources. Examples of primary data sources include surveying - remote sensing (including aerial and satellite imaging) - the global positioning system (GPS) - work logs (e.g. - police traffic crash reports) - environmental monitoring stations - and field surveys. Secondary geospatial or geospatial-temporal data can be acquired by digitizing and scanning analog maps - as well as from other sources - such as governmental agencies. The legitimacy of geographic information science as a discrete field has been claimed in terms of the unique properties of geospatial data. In a paper in which he coined the term GIScience - Goodchild (1992) identified several such properties - including: 1. Geospatial data represent spatial locations and non-spatial attributes measured at certain times. 2. The Earth`s surface is highly complex in shape and continuous in extent. 3. Geospatial data tend to be spatially autocorrelated. It has long been said that data account for the largest portion of geospatial project costs. While this maxim remains true for many projects - practitioners and their clients now can reasonably expect certain kinds of data to be freely or cheaply available via the World Wide Web. Federal - state - regional - and local government agencies - as well as commercial geospatial data producers - operate clearinghouses that provide access to geospatial data. Although geospatial data are much more abundant now than they were ten years ago - data quality issues persist. Good data are expensive to produce and to maintain. Proprietary interests simultaneously increase the supply of geospatial data and impede data accessibility. Standards for geospatial data and metadata are useful in facilitating effective search - retrieval - evaluation - integration with existing data - and appropriate uses. National and international organizations - such as the Open Geospatial Consortium (OGC) and International Organization for Standardization (ISO) - develop and promulgate such standards. INSPIRE directive (Infrastructure for Spatial Information in the European Community) regulates geospatial data management267,GD11,Satellite and shipboard remote sensing,Satellite-based sensors enable frequent mapping and analysis of very large areas. Many sensing instruments are able to measure electromagnetic energy at multiple wavelengths - including those beyond the visible band. Satellite remote sensing is a key source for regional- and global-scale land use and land cover mapping - environmental resource management - mineral exploration - and global change research. Shipboard sensors employ acoustic energy to determine seafloor depth or to create imagery of the seafloor or water column. The topics included in this unit do not comprise an exhaustive treatment of remote sensing - but they are aspects of the field about which all geospatial professionals should be knowledgeable.270,GD2-2,Remote sensing,Aerial imagery has been the primary source of detailed geospatial data for extensive study areas. Photogrammetry is producing precise measurements from aerial imagery. Aerial imaging and photogrammetry comprise a major component of the geospatial data production. Satellite-based sensors enable frequent mapping and analysis of very large areas. Sensing instruments are able to measure electromagnetic energy at multiple wavelengths. Satellite remote sensing is a key source for regional- and global-scale land use and land cover mapping - environmental resource management - mineral exploration - and global change research. Shipboard sensors employ acoustic energy to determine seafloor depth or to create imagery of the seafloor or water column. Principles of aerial photography - oblique and vertical imagery - spatial and radiometric resolution - spectral sensitivity - principal point - distortions and displacements in aerial image - parallax - stereophotogrammetry - generation of an orthoimage from a vertical aerial phoptograph - aerotriangulation - vector data extraction from digital seteroimagery - mission planning. Use of UAV in photogrammetry. Main platforms and sensors in spatial image acquisition - active and passive sensors - LiDAR and microwave - multispectral and hypersepctral imagery - interpretation of imagery - supervised and unsupervised classification - pixel based and segmented classification - ground verification - main applications - bathymetric mapping. SENTINEL.315,IP,Image processing and analysis,Image processing and analysis comprises all relevant steps to reach from (raw) image data to [...] information via image interpretation and digital image classification. In traditional remote sensing workflows - this step follows the image acquisition process. There are two main components - i.e. (1) image processing - (2) analysis - which emphasizes the sequential nature of the process – while increasingly this dichotomy disappears.\rThe information production workflow aims at converting semantically rich - but unstructured image data into a set of classes - objects - arrangements - etc. - to enable ultimately a complete image understanding and scene reconstruction. This scene reconstruction entails a mental component (“understanding”) and a technical one - by providing standardized classification results or even beyond - dedicated information products in form of digital maps and reports - tailored to the specific application domains and use cases - in order to make informed decisions. Such information products can be maps - reports - dashboards etc. - overall it is the transformation from quantitative - semi-continuous digital numbers (“brightness”) to qualitative information using categories and figures - which can be stored and further used in a GIS environment. \rThe first part of the process entails image calibration - image correction (geometric - radiometric) - data assimilation - and any type of enhancement (contrast manipulation - filtering - etc.) which aims to better condition the information extraction part. It ends where we achieve a significant milestone in the processing milestone - remarkably denoted as analysis-ready data (ARD). From there - we enter into the analysis realm - classically referred to as digital image classification - the process of assigning pixels to classes. In other words - the aggregation of pixel values according to their similarity into categorical (nominal) classes. The discrimination of these classes by and large depend on application domain - and ideally - these classes match with information classes. To address the issue of ambiguity and to overcome the so-called semantic gap in image interpretation by providing a stepping-stone in the information extraction process - the strategy of pre-classification (semi-concepts) has been introduced in the literature.\rToday - boundaries between pre-processing and classification increasingly vanish - through an increasing level of automation in the pre-processing and image correction steps. In addition - new ways of analysis emerge - in particular in large time series - including image data cubes.  Instead of a processing chain - which suggests a linear – and potentially irreversible – cascade of manipulations - the automation of large parts of this part allows us to see the process more reversible and approachable from either side.322,IP1-3-1-3,RPC correction,In satellite photogrammetry to obtain the orientation mostly of satellite scene Rational Polynomial Coefficients (RPCs) are applied. They provide a compact representation of a ground-to-image geometry - that allow for photogrammetric processing without requiring a physical camera model. Model with RPC is provided with satellite image and can be improved using measurements of indirect surveying methods used for control point measurement. The RPC model for the coordinates of the image point is calculated as ratios of the cubic polynomials in the coordinates of the world or object space or ground point. \rIn photogrammetry and remote sensing - rational polynomial coefficients (RPCs) describe a specific imaging geometry model for transforming image pixel coordinates to map coordinates (thereby accounting for terrain displacement errors). A sensor model describes the geometric relationship between the object space and the image space - or vice versa. It relates 3-D object coordinates to 2-D image coordinates. RPCs are part of a general sensor model that approximates the physical sensor model. The physical sensor model represents the physical imageing process - making use of information on the sensor's position and orientation (during image acquisition). The RPC model often refers to a specific case of the RFM (rational function model) that is in forward form - has third-order polynomials - and is usually solved by the terrain-independent scenario.327,IP1-3,Geometric correction,Geometric correction is concerned with placing the reflected - emitted - or back-scattered measurements or derivative products in their proper planimetric (map) location so they can be associated with other spatial information. It is usually necessary to preprocess the remotely sensed data and remove the geometric distortions so that individual picture elements (pixels) are in their proper planimetric (x - y) map locations. This allows remote sensing-derived information to be related to other thematic information in geographic information systems (GIS) or spatial decision support systems (SDSS). Geometrically corrected imagery can be used to extract accurate distance - polygon area - and direction (bearing) information.\r\rGeometric correction techniques are dedicated to resolving the geometric distortions caused by: (1) variations in sensor position; (2) Earth curvature; (3) rotation of Earth on its axis; (4) relief displacement. \r\rThere are two types of geometric distortions - namely systematic and random distortions. The former might be caused by Earth's rotation for example and - therefore they are predictable and systematic. The second type of distortions might be caused by terrain or variations in sensor altitude. \rGeometric correction includes georeferencing and orthorectification techniques.329,IP1-4-2,Histogram,The histogram is a useful graphic representation of the information content of a remotely sensed image. Histograms for each band of imagery are often displayed and analysed in many remote sensing investigations because they provide the analyst with an appreciation of the quality of the original data (e.g. whether it is low in contrast - high in contrast or multimodal in nature. [...] Tabulating the frequency of occurrence of each brightness value within the image provides statistical information that can be displayed graphically in a histogram.334,IP1-7-1,Atmospheric correction,Atmospheric correction accounts for the attenuation caused by scattering and absorption in the atmosphere. It transforms top-of-atmosphere (TOA) reflectance to bottom-of-atmosphere (BOA) reflectance.\rThe decision to perform atmospheric correction depends on the need - i.e. the envisioned usage of the derived EO information product and the nature of the underlying problem. This includes requirements to the accuracy of extracted biophysical information. Additionally - the decision and choice of methods depends on the type of remote sensing data available - the amount of in-situ historical and/or concurrent atmospheric information available.\rAn atmospheric correction is essential when biophysical or geophysical parameters (e.g. of water or vegetation) are going to be extracted from the remote sensing data. If the data is not corrected - the subtle differences in reflectance among the contributing image bands may be lost. This is especially relevant when biophysical information shall be compared to that of images from other dates.\rHowever - some cases exist where it is unnecessary to perform atmospheric correction. For example - it is not necessary for producing an image classification product from a single date of remotely sensed data. If a maximum likelihood classification is applied that uses training data with the same relative scale for the pixel values - then - atmospheric correction has little effect on the classification accuracy. The same holds true for a post-classification change detection where the classifications of the two different dates were performed independently. \rThe process of (absolute) atmospheric correction requires a model atmosphere and in situ atmospheric measurements acquired at the time of remote sensor data acquisition as input. In situ data can be available from other sensors on-board the sensor platform.\r\rDark Object Subtraction (DOS) is one of the most popular empirical atmospheric correction techniques. This technique assumes that a black object has a reflectance value of zero. Yet - a dark object present in a satellite image will have a value different than zero because of the atmospheric scattering. This value is then subtracted from all pixels in a given spectral band.336,IP1-7-2,Dimensionality reduction,The number of spectral bands assocuates with a remote sensing system is referred to as its data dimensionality. Hyperspectral remote sensing systems such as AVIRIS ans MODIS obtain data in 224 and 36 bands - respectively. The greater the number of bands in a dataset (i.e. - its dimensionality) - the more pixels that must be stored and processed by the digital image processing system. Storage and processing consume valuable resources. It is necessary to reduce the dimensionality of hyperspectral data while retaining the information content inherent in the image. On method to reduce dimensionality of hyperspectral data and minimizing the noise in the imagery is the minimum noise fraction (MNF) transformation (Green et al. - 1988).338,IP1-7-4,Noise reduction,As an optical remote sensing system is not perfect - noise can enter the data collection system at several points. Necessary corrections include the removal of shot noise (random bad pixels) - correcting line or column drop-outs - accounting for line-start problems and radiometric correction of n-line striping caused by detector miscalibration.\rSAR data have global - random speckle noise. Speckle filters are designed to adapt to local image variations in order to smooth values - thus reducing speckle and enhancing lines and edges to maintain the sharpness of an image. A widely used way to reduce speckle is to apply spatial filters to the images. Typical approaches for speckle filtering include Laplace filtering for smoothing and sigma filters that preserve more of the signal with a lesser effect of smoothing.340,IP1-7,Radiometric calibration and correction,Radiometric calibration and correction converts the sensor’s digital numbers (DNs) to radiance values and subsequently reflectance values. Additionally - the term “correction” points to the fact that radiometric measurements with satellite sensors contain error. Therefore - radiometric correction is concerned with improving the accuracy of surface spectral reflectance - emittance - or back-scattered measurements obtained using a remote sensing system. The Earth’s atmosphere - land and water are complex and can never be captured perfectly because of the limitations of remote sensing devices that lie in their spatial - spectral temporal and radiometric resolution. Therefore - error occurs in the data acquisition process and degrades the quality of remotely sensed data. The most common errors in remote sensing are radiometric and geometric. This concept is focused on the correction of remote sensing data to account for radiometric error that is to some degree systematic. Systematic errors in radiometric measurements come from the interaction of the sensed radiance with the atmosphere - the acquisition geometry in relation to the radiance source (the sun) and the Earth surface geometry (terrain).\rThere are several levels of radiometric calibration and correction. The first is sensor calibration that converts the DNs to top-of-atmosphere (TOA) reflectance. It converts to radiance values and further to reflectance values by accounting for the viewing angle and sun angle during acquisition. The second is atmospheric correction that converts TOA reflectance to bottom-of-atmosphere (BOA) reflectance. The third is topographic correction that converts BOA reflectance to surface reflectance. \rRadiometric calibration is necessary to ensure radiometric comparability of the measurements. There is a need for calibration when comparing different spectral bands within one image - e.g. for the calculation of geo-biophysical parameters with band math operations. Results from uncalibrated image data would differ from results achieved with calibrated data because the unaccounted cal_gain and cal_offset of the used spectral bands would lead to distortions. \rIn addition - radiometric calibration complements the geospatial comparability that is achieved with geo-referencing an image to geographic coordinates. Geo-referencing enables comparison of an image pixel to the geospatially matching pixel in another image acquired with a different sensor but with comparable resolution. Radiometric calibration enables a radiometric comparison between these two pixels’ radiance values. In case the two images are from different acquisition dates - a calculated radiometric difference would indicate change. This example shows the relevance of radiometric calibration for inter-sensor comparisons.\rRadiometric comparability is particularly relevant in studies that require inter-sensor comparisons - comparisons of surface features over time - or comparisons to laboratory or field reflectance data. Then the radiometric correction should cover atmospheric - solar and topographic effects. A full radiometric correction that also includes topographic correction can benefit the accuracy of image classifications by reducing the internal variability of vegetation types - since the corrected reflectance relates better to the geometrical or biological properties of the plant than to the original reflectance.350,IP2-3,Data integration,Data integration is the process of combining different geographic datasets including those derived from remote sensing data. The combined datasets can have different coverage - but they have to have the same geographic coordinates.351,IP2,Data assimilation,Data assimilation is a strategy to foster data integration and data harmonisation in a bi-directional way between the measured and the modelled reality. In other words - it aims to combine measurements (observations) with the understanding of the spatio-temporal properties and evolution of system’s variables or properties and model information about them. Models can be calibrated and keeping them ‘on track’ by constraining them with observations. Vice versa - observations can be validated through models. Approached as a mathematical problem - data assimilation aims at minimizing cost functions or penalize a function to ensure optimality in fitting. Equations are used to describe system parameters and the relationships among them - It is noteworthy - that models encompass information from previous measurements - experiences - and theory. While the observations are influenced by (known) properties such as precisions - etc. of the measurement devices - the robustness of models rely on the consolidated knowledge. Because uncertainties reside in all components with unknown or even undeterminable errors - the approach is usually probabilistic - including Bayesian and other related techniques.  Widely used in meteorological sciences - successful data assimilation has been boosted the reliability of weather forecast  - while sensitivity to errors remains. \rIn Earth observation - data assimilation compensates for the fact that a specific site could be observed in a variety of measurements by satellites with different sensor types - at different dates - different angular geometries and viewing directions - illumination conditions (solar time) - observation frequencies - etc. In particular - for monitoring processes - measurements over time need to assure to actually measure the status of the system or object and not the divergence in observation. To overcome these divergences and converge them with the actual properties of an observed object or target class such as spectral or geospatial properties - observation modelling can be considered an important contribution from geospatial theory. this also links to class modelling or geon modelling. The synergy of a vegetation growth model and a remote sensing observation model can be exploited to improve the retrieval of geo-biophysical information. For vegetation and crop type monitoring radiative transfer modelling (RTF) is being used as an example. \rData assimilation can also serve in bridging the gaps between non-availabilities of EO data and other observations - to provide estimates or prediction for geographical variables - testing of hypotheses or continuous observation (monitoring). A related aspect is data imputation - i.e. filling gaps in observations e.g. by other - complementary data sets (e.g. Radar imagery in the absence of VHR data in cloudy weather conditions). Recently - these sources can also be complemented by crowd mapping and citizen science. \rWhen interpretation of data comes into play - such as image classification - we introduce another level of uncertainty. Thus the community seeks for rigorus classifiers based on solid spectral models - acting across sensors. Semantic enrichment of satellite data is a related strategy for reaching to interpreted data in a rigorous way. \rSummarizing - data assimilation comprises steps to improve the level of interpretability of the input data - by enrichment (get rid of spatial/temporal gaps) - by accounting for heterogeneity (through harmonization) - and by integration (combination with other data that is relevant to the application). Thereby - datasets become more comparable to each other.352,IP3-1-1-1,Vegetation fraction,Vegetation fraction (VF) is defined “as the percentage of vegetation occupying a pixel as viewed in vertical projection. It’s a comprehensive quantitative index in forest management and vegetation community cover conditions - and it’s also an important parameter in many remote sensing ecological models.”356,IP3-1-1,Biophysical and geophysical parameters,Biophysical parameter retrieval is an approach in remote sensing that aims to estimate parameters which have physical meaning related to properties of living organisms.  The goal is to provide quantitative results directly relating to the biophysical state - but independent of acquisition conditions and technology. Assessment of vegetation status is a key motivation for this - because through plant respiration and photosynthesis - vegetation is critical for modelling terrestrial ecosystems and energy cycles in environmental studies. \rImportant parameters describing canopy structure include leaf area index (LAI) - green cover fraction (fCover) - fraction of absorbed photosynthetically active radiation (fAPAR) - plant height - biomass and leaf angle distribution.  At leaf biochemical level - leaf chlorophyll/water -  fuel moisture and leaf pigmentation content are used.\rVisual inspection can provide a first assessment of plant status. For detailed measurements of biophysical parameters - mostly destructive methods have been used. Chemical measurement techniques on leaf samples can measure pigment concentrations very accurately - but are time consuming and only use very limited samples.  \rMuch more extensive data can be collected using earth observation imagery.  These range from large scale spaceborne observations with high frequency at coarse resolution to dedicated UAV flights which can offer spectral information of  individual plants. Radar and LiDAR acquisitions - which are insensitive to weather conditions - now complement optical observations. \rMethods to retrieve the parameters from remote sensing data fall into two main categories. Statistical models empirically match data to a biophysical variable. Univariate techniques use a single quantity derived from the data - usually a vegetation index whereas multivariate techniques link a combination of measurements at different wavelengths to one or more biophysical parameters.\rPhysically-based modeling is an alternative approach which uses advanced radiative transfer models to describe the transfer and interaction of radiation inside a leaf or canopy based on robust physical - chemical - and biological processes. They compute the interaction between solar radiation and plants and provide as such a better understanding between biophysical variables and reflectance characteristics. Good examples are Leaf optical models such as PROSPECT and LIBERTY which simulate leaf optical properties by absorption and scattering coefficients. Canopy reflectance models simulate canopy reflectance as a function of a complex description of plant structural and radiometric attributes to develop a quantitative understanding of remote sensing information.362,IP3-10,Semantic enrichment,Semantic enrichment is the process of adding semantic metadata elements to improve the content-based image retrieval. These semantic metadata elements enable the explicit specification of the content of the images stored in the remote sensing databases.363,IP3-11-1,Change detection,Different types of changes are investigated using remotely sensed data: (i) abrupt changes - such as the changes caused by a fire or flooding - and (ii) gradual changes such as urban growth. Besides these kinds of changes - remote sensing community differentiates between transitional changes and conditional changes. Transitional changes refer to a major change of land surface such as conversion of forest to pasture or the expansion of mangroves into the surrounding water. Conditional changes refer to the change in condition at the surface such as water stress in an agricultural field - forest degradation caused by pest. \rIn the past - many remote sensing studies used two images to detect different types of changes such as deforestation - land cover change or change in the health or condition of the vegetation (e.g. pest infestation). Meanwhile - satellite image time series are used to assess the change. Time series analysis allows for monitoring more subtle changes and for providing temporal patterns of change. In this way - the timing of changes and drivers of change can be easily identified. \rDifferent methods are being used in change detection studies. There are studies that analyze individual images available in the investigated time series to map the target class/phenomena/events at the time when images were collected and to identify the changes: e.g. mapping the mangroves extent on an year basis and measuring it to identify changes. Alternative studies search for breaks in time series for detecting changes. The breaks are used to segment the time series into before and after changes periods which are further classified using one of the existing supervised or unsupervised classification methods (K-means - fuzzy k-means - Random Forest - Support Vector Machine etc.).365,IP3-11-3,Dynamic Time Warping,Dynamic Time Warping (DTW) works by comparing the similarity between two temporal sequences and finds their optimal alignment - resulting in a dissimilarity measure. In the case of remote sensing data - DTW can deal with temporal distortions - and can compare shifted evolution profiles and irregular sampling thanks to its ability to align radiometric profiles in an optimal manner367,IP3-12-1,Error propagation,Remote sensing-derived products such as land-use and land-cover maps contain error. The error accumulates as the remote sensing data are collected and various types of processing take place. An error assessment is necessary to identify the type and amount of error in a remote sensing-derived product.388,IP3-4-3,Classification schemes (taxonomies),Long-term monitoring of land cover and land use are particularly relevant for land ecosystem monitoring. Therefore - baseline datasets are necessary that allow assessing changes of land cover and land use where the class definitions remain consistent over time. Accordingly - classification schemes have been established that adhere to taxonomically correct definitions of classes of information organized according to logical criteria. If hard classification is to be performed (i.e. without fuzzy class boundaries) - the classes in the classification system should normally be mutually exclusive - exhaustive - and hierarchical. Mutual exclusive classes have no taxonomic overlap and assign a land cover patch to a single class. An exhaustive classification scheme is able to cover the area of interest comprehensively and leaves no land cover patch unassigned. A hierarchical system allows combining sub-classes into higher-level categories.\rFrom a remote sensing classification perspective - it becomes clear that a classification scheme consists of information classes defined by human beings. Conversely - spectral classes are those inherent to EO data. An analyst must identify spectral classes and label them as information classes that satisfy bureaucratic (or scientific requirements). Additionally - the advantage of using established classification schemes is that their use in scientific studies and applications produces results that are comparable to other studies and suitable for sharing of data.\rEstablished classification schemes include: CORINE land cover (CLC) - Land cover classification system (LCCS) - American Planning Association land-based classification standard - United States Geological Survey land-use/land-cover classification system for remote sensor data - U.S. Department of the Interior Fish & Wildlife Service classification of wetland and deep water habitats of the United States - U.S. National Vegetation Classification system (NVCS) - International Geosphere-Biosphere Program IGBP Land cover classification system.390,IP3-4-5-1,Production system,A production system performs automatic transformation of remote sensing imagery into useful information (such as biophysical parameters - categorical maps etc). An example can be a preliminary pixel-based classifier that works top-down (deductive - physical model-driven - prior knowledge-based) and arrives at preliminary classes for each pixel of an image. Such a production system does not require interaction of an operator. The process makes use of a decision tree that encodes the prior knowledge for assigning pixels to a class.391,IP3-4-5,Decision trees,Decision trees is a data mining technique used in different disciplines including Remote Sensing. It uses a tree-like prediction model to identify a pattern in the input data. One of the most popular decision tree algorithms is the CART (Classification and Regression Tree) algorithm.426,IP3-8-2,Radiative transfer modelling,Radiative transfer models describing the interaction between matter and electromagnetic radiation serve as cornerstones for optical remote sensing. The radiative transfer theory provides the most logical linkage between observations and physical processes that generate signals in optical remote sensing. Radiative transfer modelling is therefore an integral part of  remote sensing - since it provides the most efficient tool for accurate retrievals of Earth properties from satellite data. Radiative transfer models  are used in a number of different applications such as sensor radiometric calibration - atmospheric correction and the modelling radiation processes in vegetation canopies. \rVegetation radiative transfer models (RTMs) study the relationship between leaf and canopy biophysical variables and reflectance - absorbance and scattering mechanisms. The infinite variability of vegetation structure complicates the modeling of RT in vegetation canopies. Numerous models of RT in vegetation canopies were developed in the second half of the last century. Models differ by the details accounted for and by the simplifications introduced in the description of canopy structure and photon–vegetation interactions. Gradual improvement in RTMs accuracy - yet in complexity too - have diversified RTMs from simple turbid medium RTMs towards advanced Monte Carlo RTMs that allow for explicit 3D representations of complex canopy architectures. This evolution has resulted in an increase in the computational requirements to run the model - which bears implications towards practical applications. When choosing an RTM - a trade-off between invertibility and realism has to be made: simpler models are easier to invert but less realistic - while advanced models more realistic but require a large amount of variables to be configured. The two most widely used models are the leaf model PROSPECT and Scattering by Arbitrary Inclined Leaves (SAIL) canopy model. \rAtmosphere RTMs study the interaction of radiation with the atmosphere. The remotely-sensed signals at satellite or airborne platforms are combinations of surface and atmospheric contributions - with relative amounts varying across the two wavelength regions - depending on the condition of the atmosphere.  The order of magnitude of atmosphere signals can be equal or larger than that of land or ocean surface signals that arise at the top of the atmosphere (TOA). In order to derive accurate sensor calibration and atmospheric correction - the contribution of the atmospheric constituents to the total retrieved signal must be understood and modelled. Atmospheric radiative transfer models simulate the radiative transfer interactions of light scattering -  absorption and emission through the atmosphere. Some widely used atmospheric RTMs are 6SV - libRadtran - MODTRAN - and ATCOR.\rAdvances in radiative transfer modeling enhance our ability to detect and monitor changes in our planet through new methodologies and technical approaches to analyze and interpret measurements from air- and space-borne sensors.439,IP4-2-1-1,Error matrix,To correctly perform a classification accuracy (or error) assessment - it is necessary to systematically compare two sources of information: (1) pixels or polygons in a remote sensing-derived classification map - and (2) ground reference test information (which may in fact contain error). The relationship between these two sets of information is commonly summarized in an error matrix (sometimes referred to as contingency table or confusion matrix). Indeed - the error matrix provides the basis on which to both describe classification accuracy and characterize errors - which may help refine the classification or estimates derived from it.441,IP4-2-1-3,Ground reference,Ground reference refers to the reference dataset for an accuracy assessment of a remote sensing classification. The process of obtaining ground reference is dedicated to support the production of suitable accuracy information. A sampling design (fitting to the produced image classification) determines the most appropriate distribution of sample locations (or regions). The response design consists of the evaluation protocol and the labeling protocol. The evaluation protocol initiates selecting the support region on the ground (represented by a pixel or polygon) where the ground information will be collected. Once the location and dimension of the sampling unit are defined - the labelling protocol is initiated and the sampling unit is assigned a hard or fuzzy ground reference label. This ground reference label (e.g. forest) is paired with the remote sensing-derived label (e.g. - forest) for assignment in the error matrix.442,IP4-2-1-4,Kappa statistics,Kappa is a value for measuring the overall accuracy of a classification that accounts for randomness of class assignment. Kappa analysis is a discrete multivariate technique of use in accuracy assessment. Kappa yields a statistic - ^K - which is an estimate of Kappa. It is a measure of agreement between the remote sensing-derived classification map and the reference data as is indicated by a) the major diagonal and b) the chance of agreement - which is indicated by the row and column totals in the error matrix.446,IP4-2-2,Timeliness,The implementation of a service that provides remote sensing derived information on a regular basis introduces process-related quality criteria like the timeliness of information provisioning. For the case of refugee camp mapping - timely arrival of map information may be critical to support the decisions in planning facilities for humanitarian assistance.448,IP4-2-3-2,Consistency,In remote sensing we can speak about spatial consistency in the Consistency cluster. It represents the quality of image interpretation/understanding: how are the different objects or classes recognized/evaluated integrally. A bridge above a water surface - like river can be detected in pixel-wised manner - but the question is how coherent they are in the output map. This phenomenon has very close to the thematic consistency - where the recognition integrity is represented in this way. The topological consistency is defined mainly for network-type surface objects - like roads or rivers - where the connection of all atomic segments are rated by this measure. Urban mapping focuses on the built environment objects - where e.g. house-parcel inclusions are described by this feature. The temporal consistency is for monitoring again - representing for example the possibility or impossibility of land cover changes in time. Having multiple data sources (even airborne or terrestrial) - their integral usage can be qualified by this measure.452,IP4-3-1,Cloud cover percentage,The cloud cover percentage indicates the amount of area in the remote sensing image extent that is covered with clouds and therefore cannot provide information about the Earth surface conditions.The actual types of clouds included may depend on the product - but the CEOS definition includes cloud shadow. Next to that - from an optical remote sensing point of view - clouds can be roughly classified in: opaque/dense clouds - mainly composed of droplets that are highly reflective in the VIS region and generally located at low-medium altitudes and cirrus - consisting of a large number of thin non-spherical ice crystals that are normally translucent in the VIS region - relatively highly reflective in the SWIR spectrum - and located at high altitude.\r\rThe goal of cloud cover percentage is to provide a quality measure of usable information in a surface reflectance image. Earth observation product catalogs support it as a query parameter - to enable searching for products with a cloud cover percentage below a given threshold.\rThis simplifies for instance use cases that require only fully clear products (0% cloud cover) - and may save download and processing resources by only handling images that have some valid pixels. For instance - by only using products with a cloud cover percentage smaller than 99.95%. The measure also gives an estimate of the number of valid observations in a given geographical area - allowing a quick assessment of whether minimal data requirements for a specific use case are met.\r\rThe measure is a percentage of actual observations in an image - so pixels where no data was recorded are not included. For derived products - cloud cover pixels are often also flagged separately from pixels where no data was recorded - but this may depend on the data provider. The definition specifically also includes cloud shadow pixels.\rReliable cloud cover percentages depend on good cloud and cloud shadow detection methods. Especially handling of translucent cirrus clouds is an open issue: a product that has a 100% cloud cover percentage due to cirrus clouds might still be usable for some cases - while for other cases they also render the product useless. \r\rThe used cloud detection algorithm will also affect the cloud cover percentage. A more strict algorithm will yield higher percentages compared to an algorithm that under detects clouds.\rDue to these limitations - cloud cover percentages in product metadata have a fairly high error margin. The user should take this into account when determining optimal cloud cover percentage thresholds for the use case.453,IP4-3-2,Remote sensing lifecycle,The remote sensing lifecycle structures all possible phases of the data production process - from its beginning of the data's coming to existence (that includes the sensor design prior to data collection) over storage - processing and use to archiving and deletion.454,IP4-3-3,Capability to resolve anything,The capability of a sensor or EO product to resolve anything is a function of its (spatial - temporal - spectral and radiometric) resolution and of the detail at which a geographic phenomenon of interest manifests itself in time and space. A geographic phenomenon can be named or described - georeferenced and provided with a time interval at which it exists. The geographic phenomenon of interest is the one of which a user needs information to help him make a decision. Therefore - the geographic phenomenon needs to be resolved with a low enough uncertainty and a high enough quality that allows the user to make a decision with confidence. \rFor example - let’s consider a helicopter pilot that wants to know whether a specific site is suitable for an emergency landing. The decision to perform an emergency landing may be supported with an EO-derived digital map of emergency landing sites that are flat enough (as well as large enough for the pilot’s helicopter and free of any obstacles on the surface and in the approach area). If we only focus on the flatness of the terrain - we need a digital elevation model (DEM) of high enough spatial resolution and accuracy in the Z dimension to calculate slope within acceptable levels of uncertainty. The pilot probably can tell us what degrees of slope are okay for his helicopter and tell us sites (e.g. football fields) where such a landing would succeed. However - this is only the input to an analysis of different DEMs to identify the minimum spatial resolution and accuracy in the Z dimension to model slope products and associated uncertainty to derive an emergency landing site product that fulfils the requirements. Thereby the capability of different DEMs to resolve emergency landing sites can be analysed.\rSpatial resolution is a measure of the smallest angular or linear separation between two objects that can be resolved by the remote sensing system. A useful heuristic rule of thumb is that in order to detect a feature - the nominal spatial resolution of the sensor should be less than one-half the size of the feature measured in its smallest dimension.\rOther types of resolution of an EO dataset are available that determine for various geographic phenomena under investigation whether it is possible to resolve them in the data. These are radiometric resolution - spectral resolution and temporal resolution. Radiometric resolution is defined as the sensitivity of a remote sensing detector to differences in signal strength as it records the radiant flux reflected - emitted - or back-scattered from the terrain. Spectral resolution is the number and dimension (size) of specific wavelength intervals (referred to as bands or channels) in the electromagnetic spectrum to which a remote sensing instrument is sensitive. The temporal resolution of a remote sensing system generally refers to how often the sensor records imagery of a particular area. For time-series analysis - the temporal resolution determines the time granularity for resolving processes that underlie the change that is observable between subsequent images.458,IP4,Image data quality,Data quality - in general - is the degree of data usability in relation to a specific application purpose. Assurance of data quality is of growing importance in remote sensing - due to the increasing relevance of remote sensing data in planning and operational decision of public bodies and private firms - and the huge amount of digital services (or apps) that exploit RS data. \rDifferent data quality dimensions exist according to the lifecycle phases of the remote sensing data: data acquisition - data storage - data pre-processing - processing and analysis and data visualization and delivery. Remote sensing data acquisition phase involves the following quality aspects: resolution - accessibility - spatial accuracy - temporal validity - accuracy and precision of the sensor calibration. Resolution is a multi-dimensional concept that includes the following dimensions: spatial resolution - temporal resolution - radiometric resolution - spectral resolution and temporal resolution. Temporal validity refers to the quality of an remote sensing data product in time - whereas spatial accuracy refers to the accuracy of the position of features relative the Earth.  \rData storage includes the accessibility and completeness data quality dimensions.  Accessibility includes both temporal and data accessibility. Temporal accessibility refers to the time delay between data acquisition and data delivery - whereas data accessibility refers to the availability of remote sensing data. Data completeness encompasses temporal completeness - i.e. completeness of a time series represented a phenomenon - thematic completeness - and spatial completeness which refers to the area coverage. Data preprocessing - processing and analysis phase includes consistency - completeness - temporal validity - resolution - radiometric and geometric accuracy - thematic and semantic accuracy. Thematic and sematic accuracy refers to the correctness of the remote sensing data product. The main quality dimensions of the data visualization and delivery include readability - completeness and temporal validity. \rDifferent metrics can be used to assess the quality of the remote sensing-derived information - such as the root-mean-square error (RMSE) measuring the differences between the true and measured values of the phenomenon under investigation - confusion matrix used for assessing the classification performance - producer’s accuracy - user’s accuracy or Cohen kappa. The quality of the remote sensing data per se can be assessed using Peak Signal-to-noise Ratio (PSNR) or the Universal Image Quality Index (UIQI).\rDifferent organizations are involved in the standardization of the image data and gridded data quality - including ISO/TC 211 ‘Geographic information/Geomatics’ - Open Geospatial Consortium (OGC) or the Quality Assurance Framework for Earth Observation (QA4EO) developed by the Group on Earth Observation (GEO). These organizations are responsible for developing metadata standards that are further used by the remote sensing community to document the quality of the remote sensing data. According to the QA4EO - for example - all remote sensing data products need to be accompanied by a Quality Indicator (QI) which helps users assessing their fitness-for-use.464,IP5-2,Image archives,Image archives are repositories for storing - managing and retrieving remote sensing data.468,IP5,Infrastructure,In general - infrastructures such as cyberinfrastructures or Spatial Data Infrastructures (SDIs) - allow information sharing across distributed infrastructures and communities. SDIs  have gradually changed from a pool of authoritative data shared using standardized web services to a pool where the authoritative data co-exist with data collected by volunteers and different sensors. Many efforts were dedicated to data documentation - to improving the catalogues searching techniques by means of - for example - thesauri and to sharing these data using standardized web services such as Web Map Service - Web Feature Service or Web Coverage Service. Cloud computing technologies played an important role in the implementation of sustainable SDIs due to their ability to provide on-demand computational and storage capacities over the Internet. In this way - users can easily search - find and use data shared across different online platforms.\rMore specifically - infrastructures for image processing and analysis refer to the physical and organizational facilities that allow the storage - analysis and management of the available data and products. Traditionally - this infrastructure formed a digital image processing system consisting of computer hardware with special-purpose image processing software - and peripheral input-output devices (e.g. CD or DVD drives - internet access - printers/plotters). In recent years - Earth observation is undergoing a shift to online processing making use of data cubes and vast image archives - e.g. NSF EarthCube or Digital Earth Australia - the Swiss Data Cube - the EarthServer - the E-sensing platform or the Google Earth Engine. Available infrastructures aim at sharing remote sensing data and derived products following the FAIR metrics: Findable (F) - Accessible (A) - Interoperable (I) - Reusable (R). Thus - remote sensing data have to be documented using metadata that support FAIR data principles as follows: (1) Findable: remote sensing data are findable through data documentation - i.e. metadata - that needs to include a unique identifier of the described data. Metadata can be stored in a catalog compliant to one of the available data cataloging standards such as the  SpatioTemporal Asset Catalog (STAC) compliant catalog; (2) Accessible: all data have to be openly accessible and shared using interoperable formats that allow users to find - access and reuse them; (3) Interoperable: different standards - e.g. STAC specification - have to be used to document remote sensing data; (4) Reusable: metadata have to be comprehensive enough to allow users not only to assess the fitness for purpose (e.g. lineage) but also to provide them information about how to access the generated data.503,OI5-4,Pan-European and global associations and professional organizations,The European GIS&T landscape consists of many pan-European organizations and associations promoting the interest of and representing certain stakeholder groups. While some of these organisations are dealing with all sectors and aspects of geographic information - others have a more thematic focus (e.g. remote sensing - topography - geosciences) or represent a particular sector (e.g. research - business). In some cases - their clearly is an overlap in the mission and objectives of different organizations - and some organizations are working in the same field of interest. Some examples of pan-European organizations and associations are AGILE - EuroSDR - EUROGI - and EuroGeographics. Also at international level several membership organizations and associations exist.504,OI5-5,The geospatial industry,The geospatial industry consists of companies working with location specific information or services. Within the geospatial sector - several areas of activities can be identified: 1) measuring - collecting and storing of data about geo-objects; 2) processing - editing - modelling - analyzing and managing that data; 3) presenting - producing and distributing the data; and 4) advising - educating - researching and communicating about processes and use of geo-information products and services. The sector consists of both small-and-medium-sized enterprises but also big companies - including surveyors - census hard-copy map providers - aerial photos providers - base map data providers - satellite and remote sensing imagery providers - software developers (GIS-related products and services providers as well as satellite image programming platform providers) and several others.514,PP1-1-6,Stefan–Boltzmann law. Kirchoff law,The total radiant intensity B(T ) of a blackbody at the absolute temperature T can be derived by integrating the Planck function over the entire wavelength domain from 0 to∞. Since blackbody radiation is isotropic - the flux density emitted by a blackbody is therefore F = π B(T ) which is proportional to the fourth power of the absolute temperature T through the Stefan-Boltzmann constant σ = 5.67 × 10−8 J m−2 sec−1 deg−4.\rKirchoff's law establishes that for a medium at the thermodynamic equilibrium - the spectral emissivity ε(λ) at a given wavelength λ - is equal to the its spectral absorbance - A(λ) at the same wavelength λ.   Hence ε(λ)=A(λ) at each fixed λ -  for a blackbody   ε(λ)=A(λ)=1 at whatever λ. Kirchoff's law is valid also in Local Thermodynamic Equilibrium (LTE) conditions as the ones  usually occurring in (small volumes of) the Earth's atmosphere even in the most turbulent conditions.\rKirchoff's law has important applications also for the study of spectral signatures of  mineral and rocks and - in general - of opaque - i.e. with spectral transmittance T(λ)=0 - bodies. In that case - the relation which relate the spectral reflectance R(λ) - absorbance A(λ) and transmittance T(λ) of a body: R(λ)+A(λ)+T(λ) =1\rreduce to R(λ)+A(λ)=1 and in LTE conditions - thanks to the Kirchoff's law: \rR(λ)+ε(λ)=1 which allows to obtain measurements of spectral emissivity indirectly through (more simple and stable) measurements of spectral reflectance:\rε(λ)=1-R(λ)\rRocks and mineral exhibit important (diagnostic/discriminating) signatures in their spectral emissivity in the thermal infrared (TIR) region. Measuring spectral emissivity in a laboratory (particularly if samples have to be characterized for their properties in natural conditions) is a quite difficult task due to the difficulty to insolate the sample from the lab environment (and instruments themselves) all emitting approximately at the same (environmental)  temperature. Kirchoff's law allows to obtain - for opaque bodies - spectral emissivities  from spectral reflectances measurements which are much easy to  realize in normal remote sensing labs.517,PP1-1,EM radiation,EM radiation is created when an electrically charge particle - such as an electron - is accelerated by a force causing it to move. The movement produces oscillating electric and magnetic fields which travel - as an harmonic EM wave - at right angles to each other. EM waves travel at 299 -792 -458 meters per second in a vacuum (the highest possible speed into the Universe - also known as the speed of light). \rThe electromagnetic field propagating through the space as EM waves is also referred as electromagnetic radiation. \rAn EM wave is characterized by a frequency (or by a wavelength) and by an amplitude (or by an energy). \rThe wavelength is the distance between two consecutive peaks of a wave. This distance is given in meters (m) or fractions thereof. Frequency is the number of waves that form in a given length of time. It is usually measured as the number of wave cycles per second - or Hertz (Hz). It is wave speed=frequency*wavelength so that - an EM wave traveling at the speed of light - can be equally identified by its wavelength or by its frequency. The amplitude (i.e. the maximum oscillation of the EM field) provide the intensity (i.e. the energy) of the EM wave.  \rThe classical theory describes the EM radiation as electromagnetic waves which represent the oscillations of electric and magnetic fields. In the quantum mechanics theory EM radiation consists of photons - quanta of the electromagnetic energy - responsible for all electromagnetic interactions.\rAs far as Earth remote sensing is concerned EM radiation represents the most important  vehicle of information.518,PP1-2-1,Atomic spectroscopy,The study of the absorbption/emission of electromagnetic radiation by atoms. Depending on the atomic number characteristic frequency or wavelength are absorbed or emitted. Since each element has a characteristic spectrum of absorbed/emitted wavelengths (spectral signature) - atomic spectroscopy allows the determination of elemental compositions even of remote objects (e.g. stars - galaxies - etc.).\rStarting from the simple Bohr’s model it is possible to predict quite exactly the frequencies of e.m. radiation selectively absorbed/emitted by all atoms. Depending on the atomic number Z - characteristic frequencies f are absorbed or emitted by atoms corresponding to the electronic transitions from different energetic (quantized) states following the Bohr’s condition: fab=(Eb- Ea)/h -  being Ei=-cost∙Z2/(ni)2 the electron energy corresponding to the state/level i (principal quantic number ni). By this way each atomic species has a characteristic spectrum of absorbed/emitted frequencies (atomic spectral signature) so that  atomic spectroscopy allows the determination of elemental compositions even of remote objects. By this way the existence of Helium was discovered in the 1968 by Jansen and Lockyer in the Sun photosphere well before its discover on the Earth - and the knowledge of the chemical composition of stars and galaxies was possible well before the end of XIX century. Atomic spectroscopy provides a simple and powerful introduction (through the explanation of the more complex interactions of e.m. radiation with molecules and solid matter) to the fundamental concepts of spectral signature (which is at the base of most of the applications of aerial remote sensing of the Earth’s surface) and atmospheric windows (important for the design of optical sensors devoted to remotely sense Earth’s surface) being moreover propaedeutic to the understanding of methods for the atmospheric vertical sounding based on the concepts spectral lines broadening and related weighting functions.530,PP1-2,Radiation - Matter interaction,E.M. Radiation can be absorbed - scattered - emitted and transmitted by the matter. The results of such interactions (i.e. the fraction of incident radiation that is absorbed - scattered or transmitted) or emission process (i.e. the fraction of actually emitted radiation in comparison with the one expected from a black-body at the same temerature) strongly depend on the radiation wavelength and on specific chemical (e.g. composing atoms and molecules as well as their arrangement within solid cristals) and physical (e.g. Temperature - Dimensions and Shape - Roughness) properties of the matter. In some case - the result of Radiation - Matter interaction is strongly affected by observational conditions. For instance - over some angular distance between the directions of incidence and the one of measurement of the radiation -  sun-glint can occur which completely mask any other results. A basic principle of the remote sensing put univocally in relation spectral absorbance - reflectance - transmittance and emissivity - curves achievable by multi-spectral EO measurements -  with matter having specific chemical/physical properties.  Theoretical models of radiation-matter interaction at the Earth's surface and through the atmosphere provide then suitable strategies for retrieving - from multi-spectral measurements of the radiation leaving the Earth - the most relevant chemical/physical properties of the matter composing its surface and atmosphere.535,PP1-3-5,Spectral Signature of Mineral and Rocks,Spectral signatures of rocks and mineral provide information on their chemical composition and crystal properties - grain size and roughness over a wide range of wavelengths from the visible to the thermal infrared.\rIn the Visible and Near-InfraRed (VNIR; 0.4÷1.0 µm) region - spectral features are dominated by electronic processes in transition metals - such as Fe - Mn - Cu - Ni - Cr - etc. Therefore - iron is the most important constituent having spectral properties in the VNIR - and the iron-rich minerals are characterized by low reflectance (high absorbance) below 0.7 µm.\rOther minerals - which represent the major part of the Earth's surface rocks - such us Si - Al and some anion groups (e.g. silicates - carbonates - oxides) hydroxides - have less spectral features in the VNIR region - but exhibit much more evidences in the Short-Wave InfraRed (SWIR; 1÷3 µm) region. In fact - spectral features of hydroxyls and carbonates mark the SWIR region.\rThe hydroxyl ion is a widespread constituent occurring in rock forming minerals such as clays - micas - chlorite etc. It shows a vibrational fundamental absorption band at about 2.74÷2.77 µm and an overtone at 1.44 µm.\rCarbonates - which are commonly in the Earth surface rocks in the form of calcite (CaC03) - magnesite (MgC03) - dolomite [(Ca-Mg) C03] and siderite (FeC03) - shows a typical absorbance feature around 2.3 µm - instead the water content can be instead evaluated by the depth of absorption at 1 -4µm and 1 -9 µm.\rThermal InfraRed (TIR; 1÷20 µm) region - from a geological point of view - is a particularly important spectral region for remote sensing aiming at compositional investigations of terrestrial materials. In fact - the fundamental vibration features of many rock-forming mineral groups (e.g. silicates - carbonates - oxides - phosphates - sulphates - nitrates - nitrites - hydroxyls) occur in the TIR region. Briefly:\ra) the silicates - which are most abundant group of minerals in the Earth's crust - shows vibrational spectral features due to the presence of Si04-tetrahedron around 8 µm to 12 µm; b) the carbonates show a weak feature around 11.3 µm that can be detected; c) the sulphates display bands near 9 µm and 16 µm; d) the phosphates also have fundamental features near 9.25 µm and 10.3 µm; e) the features in oxides usually occupy the same range as that of bands in Si-O - i.e. 8 µm to 12 µm; g) the nitrates have spectral features at 7.2 µm and the nitrites at 8 µm and 11.8 µm; h) the hydroxyl ions display fundamental vibration bands at 11 µm.538,PP1-3-8,Definition of active and passive remote sensing techniques,One of the most common ways to classify remote sensing systems consists in distinguishing them into the passive systems - which detect naturally occurring radiation - and the active systems - which emit radiation and analyse what is sent back to them. The passive systems can be further subdivided into those that detect radiation emitted by the Sun (this radiation consists mostly of ultraviolet - visible and near-infrared radiation) - and those that detect the thermal radiation that is emitted by all objects that are not at absolute zero (i.e. all objects). For objects at typical terrestrial temperatures - this thermal emission occurs mostly in the infrared part of the spectrum - at wavelengths of the order of 10 μm (the so called thermal infrared region) - although measurable quantities of radiation also occur at longer wavelengths - as far as the microwave part of the spectrum. Active systems can - in principle - use any type of electromagnetic radiation - resulting able to obtain measurements anytime - regardless of the time of day or season. In practice - however - they are restricted by the transparency of the Earth’s atmosphere at the specific spectral range considered. In any case they can be used for examining wavelengths that are not sufficiently provided by the sun - such as microwaves - or to better control the way a target is illuminated. Active sensors may be classified according to the use that is made of the returned signal. Two main methods have been identified to this aim so far: the Ranging technique mostly concerns with the time delay between transmission and reception of the signal - while the Scattering one is mostly focused on the strength of the received signal.552,PP1-5-1,Reflection - Refraction and Dispersion of the light,Light is the electromagnetic phenomenon we exploit for remote sensing. Its basic laws concerning the transmission through the interface of two different media are governed by reflection and refraction. Reflection governs the way light is backpropagated and refraction dictates how light is transmitted. Refraction is related to the real refractive index of a medium. Dispersion relates to the way the light of a given wavelength is transmitted. Since light of different wavelengths are transmitted at different angles - the phenomenon leads to the concept of dispersion. These three simple principles are at the core of the understanding technology of remote sensing.554,PP1-5-14,Electric conduction in solids: semiconductors - p-n- junction - diode and transistors,Solid state modern detectors rely on non-metal junction - which can be designed and operated to yield a bandgap energy according to the spectral range (infrared - visible - UV) to be detected. The basic principles of how these devices are designed and fabricated is important to develop and design new sensors useful for the various remote sensing applications.556,PP1-5-2,Interference and Diffraction.,Interference and diffraction are phenomena related to the wave nature of electromagnetic radiation. They explain how light propagates in presence of obstacles. These phenomena are largely used in the fabrications of optical systems for remote sensing: e.g. radiometers and spectrometers.571,PP1-6-5,Thermal infrared radiation transfer in the atmosphere,When we talk about “thermal infrared (or terrestrial) radiation” we commonly refer to the energy emitted from the Earth-atmosphere system. Trapping of thermal infrared radiation by atmospheric gases is typical of the atmosphere and is therefore called the “atmospheric effect”. The atmospheric effect is sometimes referred to as the “greenhouse effect” because in a similar way glass - which covers a greenhouse - transmits short-wave solar radiation - however absorbs long-wave thermal infrared radiation. Imagine a beam of radiation travelling through a small section of air. The air is made up of changing concentrations of different species - with all molecules absorbing and emitting thermal radiation at different rates. As the radiation travels through different layers of the atmosphere - the intensity of radiation will constantly be modified by both absorption and emission processes as described by the Schwarzschild's equation. In case of a sensor on board of a satellite - the net radiation measured would be that which is attenuated through each layer (as small increments of absorption and emission) from the surface to the top of the atmosphere plus the radiation emitted directly from the atmosphere. In this case - this process can be described by the radiative transfer equation (RTE). \rThe equation of radiative transfer simply says that as a beam of radiation travels through the atmosphere - it loses energy to absorption - gains energy by emission - and redistributes energy by scattering. Many radiative transfer codes exist which are able - i.e. on the basis of known properties of the atmosphere - to computed the effect of the atmosphere on the thermal infrared radiation providing atmospheric transmittance (absorption) - atmospheric scattering and atmosphere path emission. Commonly - in satellite remote sensing - the thermal infrared region is defined as the region of the electromagnetic spectrum comprised between 8 and 14 micron. In an atmosphere free of particles (aerosols due to phenomena like fires - volcanic eruption - dust storm - etc.) the thermal infrared radiation is mainly affected by triatomic gases like water vapor - carbon dioxide and ozone.573,PP1-6-7,Earth's (standard) Atmosphere Transmittance,Each time radiation passes through the atmosphere it is attenuated to some extent. We refer to this attenuation with the term 'atmosphere transmittance'. The typical atmospheric transmittance between wavelengths of 250 nm and 2500 nm - i.e. in the ultraviolet - visible - near-infrared and short-wave-infrared regions of the spectrum is dominated bywater vapour - although methane - carbon dioxide and molecular oxygen are also responsible for a few absorption lines. The behaviour in the visible region is dominated by molecular Rayleigh scattering. At the short-wavelength end of the spectrum - in the ultraviolet - absorption by ozone becomes very significant. Above 2500 nm up to the upper limit (13500 nm) of the optical electromagnetic spectrum useful for Remote Sensing - the atmosphere transmittance is mainly affected by triatomic molecules (H20 - CO2 and O3). However - the atmospheric effects (transmittance) is strongly depending on the electromagntic wavelength. Remote Sensing exploits the region of relative atmospheric transparency called atmospheric windows.574,PP1-6-8,Atmospheric (spectral) windows for EO,With the term 'atmospheric windows' we refer to the regions of the electromagnetic spectrum where the interaction between the atmosphere constituents (i.e. - molecules - aerosols - and cloud particles) and the electromagnetic radiation is minimized - namely the mechanisms of scattering and absorption of the radiation are less relevant than the transmission one. Therefore - the radiation collected at the sensor in these spectral regions is strictly depending on the Earth surface features - allowing to infer information about the processes/phenomena there in progress at the time of the acquisition. There are three main spectral ‘windows’ in the Earth's atmosphere. The first of these includes the visible and near-infrared (VNIR) parts of the spectrum up to the medium infrared - between wavelengths of about 0.38 μm and 3.5 μm - although it does also contain a number of opaque regions. This spectral interval includes the small portion of the electromagnetic spectrum to which human eyes are sensitive to (i.e - the visibile region between 0.4 and 0.7 μm). The second is a rather narrow region between about 8 μm and 15 μm - in which is found the bulk of the thermal infrared (TIR) radiation from objects at typical terrestrial temperatures. In this region there is only a main opaque interval - around 9.6 μm due to the presence of the ozone band. The third more or less corresponds to the microwave region - between wavelengths of a few millimeters and a few meters. Therefore - each remote sensing instrument that should be able to fully penetrate the Earth’s atmosphere has to be designed to operate in one of these three ‘window’ regions.594,PP1,Basics of Optical Remote Sensing,Optical Remote Sensing deals with those part of electromagnetic spectrum characterized by the wavelengths from the visible (0.4 micrometer) to the near infrared (NIR) up to thermal infrared (TIR - 15 micrometer). It regards the collection and interpretation of the e.m. radiation emitted - reflected - adsorbed and transmitted by the observed targets in order to derive their physical-chemical properties and related information. Such a possibility derives from the basic principle of (multi-spectral) remote sensing that is widely supported both theoretically (e.g. atomic and molecular spectroscopy) and experimentally (e.g. spectral signatures catalogues).     It states that - in principle (e.g. disposing of sensors with ideal spectral capabilities) the matter-radiation interaction depends on the wavelength of the  involved radiation and on specific (e.g. chemical/physical) properties of the matter that can be derived by the spectral analysis of the emerging (emitted - reflected - adsorbed or transmitted) radiation.  As far as Earth Observation is concerned - specific related concepts  have to be addressed like: the spectral  matter-radiation interactions (spectral signature concept) - natural sources (e.g. Earth - Sun) of optical e.m. radiation - theory of the Black Body - atmospheric physics and radiative transfer equations in the VIS-NIR and TIR spectral ranges - basic physics of e.m. optical sensors and image systems - physical fundaments of the interpretation of optical radiances collected by multi-hyperspectral passive  techniques.600,PP2-1-6,Phase,In remote sensing - phase is the exact position within a periodic signal with respect to an arbitrary reference point. It is typically expressed as an angle and measured in degrees or radians - where one period corresponds to a phase of 360° or 2π - respectively. Mathematically - phase is the argument of a complex number - that is the angle between its geometric representation in the complex plane and the real axis. For this reason - complex algebra is often used in remote sensing to facilitate phase calculations. Due to its periodic nature - phase can only be measured unambiguously within one period. Consequently - phase measurements are commonly subject to 2π phase ambiguities. These ambiguities can often be resolved in a process called phase unwrapping - using a priori information about the signal - typically related to its continuity. Phase measurements are crucial for the creation of synthetic aperture radar (SAR) images - as well as for many SAR imaging techniques - including interferometric SAR (InSAR).604,PP2-2-1,Diffraction,Diffraction is defined as interaction of waves with any solid object - not surfaces - and is not to be confused with refraction. More precisely - diffraction describes the phenomena of interaction of waves at an obstacle - such as an aperture - or an opening - such as a hole or an occurring space between two objects. Hence - diffraction is an essential form of scattering - describing ordered scattering at discrete boundaries. The effect of diffraction can be observed through extended interference patterns or simply by the bending of waves. In the field of microwave remote sensing - diffraction has the practical implication that it limits the spatial resolution of a microwave sensor since it acts on the ability of an imaging system to resolve details. This theoretical limit of resolution is called the diffraction limit. This means - the larger the aperture of the observing system compared to its employed wavelength (dependent on the frequency) - the finer the resolution of an imaging system. The diffracted field can be calculated with analytical models - such as the Fraunhofer diffraction approximation in case of far field conditions - where the object is far away and the incident waves are assumed to be plane waves - or the Fresnel diffraction approximation in case of near field conditions - where the waves are spherical.\rOne simple example of diffraction is the diffraction of sound - for example the possibility to hear sounds around corners.605,PP2-2-2,Scattering and emission,Scattering means the redirection of incident electromagnetic energy by an object. Similar to diffraction - scattering refers to the same physical process - the coherent distortion of an incident wave. However - diffraction as well as reflection can be regarded as essentially forms of scattering. Scattering explicitly describes the “random distortion of waves by elements that are similar in size or less than the wavelength” (Woodhouse - 2005). Thereby - scattering of the incident wave at an object can occur in any directions with varying strength - with the scattering pattern varying with the incident direction. Thus - the term scattering cross section - often denoted by σ - quantifies the effectiveness of a scatterer. In the field of active microwave remote sensing - the backscattering coefficient σ0 is known “as the ratio of the statistically - averaged - scattered power density to the average incident power density” (Fung - 1994). \rIn passive microwave remote sensing - radiometers measure the intensity of radiation emitted by a body - called brightness temperature TB. Since TB is always less than its physical temperature T - emissivity - defined as e = TB / T - is a measure of how strongly a body radiates at a given wavelength. It varies between 0 (metal) to unity (blackbody).\rEmission and scattering are complementary: surfaces that are good scatterers are weak emitters - and vice versa.622,PP2-2-6,Surface roughness,Surface roughness defines the geometry between the pedosphere and the atmosphere (soil-air boundary).\rIn the field of microwave remote sensing - surface roughness affects scattering and emission characteristics of natural surfaces. The degree of roughness of a random surface is determined by statistical parameters - measured by the units of wavelength of the observing sensor. The two fundamental surface roughness parameters are the standard deviation of the surface height variation (RMS height) s - with its related surface correlation function p(ξ) - and the horizontal surface correlation length l. Additional - a third roughness parameter - the root-mean-square (RMS) slope m - is important for some surface scattering models to simulate electromagnetic wave scattering of surfaces.\rSurface roughness determines the variation of surface height within an imaged resolution cell. The transition from smooth to rough is qualitative - and is function of both wavelength and incident angle. With decreasing frequency the soil surface appears rather smooth to microwave sensors. This results in the fact - that while one surface appears smooth when sensed at L-band (λ ≈23 cm) - the same surface appears rough when sensed at X-band (λ≈3 cm). Hence - in the field of microwave remote sensing - the ‘effective’ surface roughness parameters are scaled by the wave number k= 2π/λ. Surface roughness can be observed at single or multi-scale.632,PP2-2,Interaction of microwaves with matter,A number of interactions are possible when electromagnetic energy encounters matter - whether solid - liquid or gas. In Earth Observation there are two main interactions: atmospheric and with target. Atmospheric interaction: In radar remote sensing - atmospheric interactions are limited due to the long wavelengths compared to the size of the atmospheric particles. The fact that microwaves interact with object at least as big as the wavelength is one of the greatest advantages of microwave remote sensing - since at larger wavelengths atmospheric particles are almost transparent to the signal and microwave sensors are independent from the time of day (day or night) and weather conditions. Water clouds can interfere with the radars operating below 2 cm in wavelength. The effects of rain can be generally ignored at wavelengths above 4 cm. For longer wavelengths (above 20 cm) - an effect called Faraday rotation caused by the ionosphere - i.e. - free charges (electrons) and the Earth’s magnetic field - can lead to a rotation of the polarization plane. Target interaction: The radar interaction with the object is a result of both radar system parameters (frequency - polarization - acquisition geometry) and the physical properties of the object (dielectric constant - i.e. - water content; geometrical properties - i.e. - the roughness - shape and orientation of the scatterer). Overall - various types of interactions can be distinguished – scattering - diffraction - and reflection – all describing the same process of wave interaction but at different scales.643,PP2-3-11,Principles of Synthetic Aperture Radar Interferometry (InSAR),Synthetic aperture radar (SAR) interferometry - or simply InSAR - is a remote sensing technique utilising the phase difference between two or more complex-valued SAR images. Most modern SAR systems are capable of measuring both the intensity and the phase of the reflected signal - where the latter carries information about the distance travelled by the signal. Consequently - the different of phase information of two successive SAR images over a specific area contains a distance information. \r\rThe phase difference measured between two SAR images is called the interferometric phase. The interferometric phase image is an interferogram. The interferometric phase is a function of the geometry and timing of the individual SAR acquisitions. Different geometric and temporal configurations enable different applications. \r\rIf the SAR acquisitions are made from different angles and without significant temporal change of the scene - InSAR can be used to create digital elevation models (DEMs) of the Earth - as demonstrated by the NASA/JPL Shuttle Radar Topography Mission (SRTM). This configuration is called across-track interferometry. If the individual SAR acquisitions are made at different times in the same geometric configuration - i.e. in an along-track or differential interferometric configuration - then InSAR can be used to measure radial velocity of targets and to assess displacements caused by - e.g. - volcanoes and earthquakes. The variation of the temporal baseline allows determining velocities ranging from several meters per second to a few millimeters per year. While standard differential interferometry can be used to retrieve changes that happened between two SAR acquisitions - differential interferometric stacking techniques - such as Persistent Scatterer Interferometry (PSI) and Small Baseline Subset (SBAS) - are used to monitor deformation over a longer period of time by stacking multiple differential interferograms and filtering out the atmospheric phase contribution in order to retrieve very accurate deformation of the ground and its infrastructures.648,PP2-3-5,Real Aperture Radar (RAR),There are two types of imaging radar apertures: real (usually called RAR or SLAR for side-looking airborne radar or SLR for side-looking radar) and synthetic aperture radar (SAR). The SLAR imaging system uses a long antenna mounted on a platform. The synthetic aperture is used in space remote sensing applications. RAR is a radar system where the antenna beamwidth equals to the physical length of the antenna. It operates in a side-looking configuration - left or right with reference to the flight direction. It is an active - all-weather - day/night remote sensor onboar an airborne platform. Both Real Aperture and Synthetic Aperture Radar are side-looking systems having antennas aimed to the right or left of the flight path. The length of the antenna together with wavelenght determines the resolution in the azimuth direction - i.e. it is proportional to the distance to the object and inversely proportional to the length of the radar antenna.662,PP2-3,Detecting microwaves,Microwave remote sensing systems detect and quantify the electromagnetic radiation arriving at a detector - this radiation being either emitted (passive sensors) or scatterered back (active sensors) from the objects.\rThree properties of the recorded electromagnetic signal are of particular interest: its intensity - its phase and its polarization. The specific quantification of each properties allows signal interpretation - as they depend on the roughness and dielectric characteristics of the surface (intensity and polarization) as well as of the range between target and sensor (phase).\rThe detection of the microwaves is operated through two principal sensor elements: an antenna and a receiver. The antenna collects the incoming radiation and the receiver measures the collected electric signal.\rAs active microwave systems produce their own electromagnetic radiation - they are equipped with two additional elements: a pulse generator and a transmitter. Usually - transmitter and receiver are situated on the same antenna.\rA simple detector system only detects the intensity of the signal and amplifies it. Coherent systems measure both the amplitude and the phase of the incident electromagnetic radiation.\rMicrowave systems can be categorized in two different types: imaging and non-imaging sytems. Whereas for non-imaging systems each echoe (collected signal) provides a single measurement - imaging systems collect a sequence of echoes that generate a two dimensional image.663,PP2,Basics of microwave remote sensing,Microwave remote sensing operates in the microwave portion of the electromagnetic spectrum - generally using wavelengths greater than 3 cm and up to 1 m. \rMicrowaves are sensitive to different physical parameters than other regions of the electromagnetic spectrum. Microwaves interactions with objects are governed by geometric (structure - size - shape) and dielectric (water content) properties - whereas other regions of the electromagnetic spectrum reacts e.g. to object temperature or “color” (amount of reflection or absorption of the Sun light by a particular object).\rAs a general rule - microwaves interact with object at least as big as the wavelength. Smaller objects will therefore be transparent for the signal. Due to the large wavelengths - atmospheric particles are almost transparent to the signal and microwave remote sensing can penetrate clouds. Under very dry conditions - microwaves can even penetrate up to a few meters the top soil layers - therefore providing information that is not visible in other regions of the electromagnetic spectrum. Depending on the considered wavelength - microwave can also penetrate vegetation layers to different amounts.\rIn microwave remote sensing - three characteristics of the electromagnetic wave play an important role: its amplitude - its phase and its polarization. Depending on the application - either one characteristic or a combination of them is used to retrieve information.\rThere are two main types of microwave sensors: active RADAR systems and passive radiometers. RADAR is an acronym for RAdio Detection And Raging. An active radar system sends out pulses and records the echoes scattered back by the objects (scatterers) to the sensor. The systems use the two-way travel time of the radar pulse to determine the distance (range) to the illuminated object. Its backscatter intensity is determined by the radar system and object properties and depends on the quantity of energy coming back to the sensor. Active radar systems transmit a signal and record the amount of energy that is scattered back and depends of both dielectric and geometric properties.  Passive radiometers record microwave energy - which is emitted by the Earth’s surface.\rDepending on the type of system - microwave remote sensing can be used in multiple applications. Active sensors are principally used for diverse land cover mapping applications based on the particular backscattering mechanisms and characteristics of the objects on the Earth’s surface. Using multiple acquisitions - they are also favored for topographic - deformation and velocity mapping. Passive sensors are preferred for the determination of hydrologic variables such as soil moisture - precipitation - ice water content and sea-surface temperature.664,PS,Platforms - sensors and digital imagery,Remote sensing - i.e. the process of obtaining information about an object or area from a distance - is not possible without remote sensing sensors that collect this information and the platforms on which the sensors are installed and which are used to move them. Remote sensing sensors collect data by detecting energy that is reflected or emitted from Earth. There are different types of remote sensing sensors. The interaction between the sensor and the Earth's surface has two modes: active or passive. Passive sensors use solar radiation to illuminate the Earth's surface and detect reflection from the surface or measure the emitted energy. They usually record electromagnetic waves in the visible (˜430–720 nm) and near infrared (NIR) (˜750–950 nm) through short infrared (SWIR) (˜1.500-2.500 nm) to thermal infrared (TIR) (8.000-14.000 nm) ranges. The power measured by passive sensors is a function of surface composition - physical temperature - surface roughness and other physical properties of the Earth. Active sensors provide their own energy source to illuminate objects and measure their properties. These sensors use electromagnetic waves in the visible and near infrared range (e.g.laser altimeter) and radar waves (e.g. synthetic aperture radar (SAR)). As sensor technology has advanced - the integration of passive and active sensors into one system has emerged. Alternatively - remote sensing sensors can be classified into imaging sensors - i.e. that produce an image of an area - within which smaller parts of the sensor's whole view are resolved (pixels) - and non-imaging sensors - i.e. that return a signal based on the intensity of the whole field of view. In terms of their spectral characteristics - the imaging sensors include optical imaging sensors - thermal imaging sensors - and radar imaging sensors. These sensors can be on satellites - mounted on aircraft - unmanned aerial vehicle (UAV) -  drone or ground. The collected information can be transformed into an image or set of points (e.g. cloud points) - which can be further processed and analyzed to obtain the necessary information - e.g. agricultural field development phase - level of air pollution - etc.\rA digital imagery of Earth observation sensors is a two-dimensional representation of objects on Earth. Current images collected from different levels of acquisition - from ground to satellite - with the help of electronic sensors are examples of digital images. There are different aspects and characteristics of remote sensing data and images - such as - for example - data formats and processing levels - data storage - data properties.665,PS1-1,History of remote sensing sensors,Remote sensing sensors has its roots in the 19th century in the development of photography. Photography was an invention that made it possible to acquire a permanent image. The first photographic image was taken in 1826 by Joseph Nicephore Nieppce. While the first aerial photograph was taken in 1858 by Felix Tournachon - known as Nadar - from a tethered baloon over Biévre Valley in France. In 1907 Julius Neubronner developed a light miniature camera that could be fitted to a pigeon's breast. It can be said that the construction camera + pigeon was the precursor of today's unmanned aerial vehicle (UAV) or drone. Further developments focused on developing new sensors (analog vs. digital frame cameras) and how to save and store images (e.g. photographic emulsions - films). The origin of other types of remote sensing can be traced to World War II - with the development of radar - sonar - and thermal infrared detection systems. Since the 1960s - sensors were designed to operate in virtually all of the electromagnetic spectrum. Both civil and military aerial photography have long been widely used in cartography to create maps. Specialized large format cameras (looking vertically down - assuming the plane is flying horizontally) were developed. Such cameras have been specially designed to perform almost vertical sequences of bird-eye exposures during aircraft flight. Hence for a long time remote sensing consisted of aerial photography and photogrammetry using analogue mechanical or optical equipment. Everything has changed with satellites and the space race. The first real success of remote sensing satellites in serious scientific work was in meteorology - weather satellite TIROS-1 - launched by NASA on April 1 - 1960. \rToday a wide variety of remote sensing instruments are available as data source for use in different applications for land - water and atmosphere monitoring.666,PS1-2-1-1-1,Along track scanners,Along track scanner - also known as a pushbroom scanner - is an optoelectronic device that obtains images with a multispectral imaging system. The scanners are used for passive remote sensing. It records electromagnetic energy that is reflected (e.g. - blue - green - red - and infrared light) or emitted (e.g. - thermal infrared radiation) from the surface of the Earth. The scanners are mounted on space- or aircrafts. \rA two-dimensional image is created (line by line) by exploiting the platform motion along the orbital track. The data are collected along track using a linear array of detectors arranged perpendicular to the direction of travel. The array of detectors are pushed along the flight direction to scan the successive scan lines - and hence the name pushbroom scanner. \rThere are no moving parts on a pushbroom sensor - hence - the scanning speed can be increased compared to across track systems. A longer dwell time over each ground resolution cell increases the signal strength (high radiometric resolution - no pixel distortion). Additionally - finer spatial and spectral resolution can be achieved as the size of the ground resolution cell is determined by the Instantaneous Field of View (IFOV) of a single detector. The systems are designed for high-resolution imaging. However - a very large number of detectors is needed for high resolution images. It is a complex optical system. In addition - the pushbroom scheme requires a wide Field of View (FOV) optics system to obtain the same swath as for a corresponding whiskbroom (across track) scanner. It has narrow swath width.     \rThe detector arrays with such a line-scanning pushbroom system are usually of the type Charge-Coupled Device (CCD).\rThe MultiSpectral Instrument (MSI) on board the Sentinel-2 satellite (Copernicus mission) uses a pushbroom concept.\rMultispectral imaging systems building the final image (line by line) exploiting the platform motion along the orbital track. No rotating mechanical part required - usually based on a CCD matrix (high spectral resolution but just up to 1 micrometer) - e.g. Sentinel-2 MultiSpectral Instrument (MSI) - Sentinel-3 Ocean and Land Colour Imager (OCLI).676,PS1-2,Passive vs. active sensors,Passive remote sensing systems record electromagnetic energy that is reflected (e.g. - blue - green - red - and infrared light) or emitted (e.g. - thermal infrared radiation) from the surface of the Earth. Passive sensors therefore rely on an external energy source (e.g. sun illumination - Earth heat emission). Contrary to passive sensors - who detect naturally occurring radiation - active sensors emit radiation and collect and analyze the signal that is sent back by the Earth’s surface or atmosphere. Active remote sensing systems produce therefore their own electromagnetic energy. They transmit and receive the radiation that is reflected or backscattered from the illuminated target. They do not necessitate an external source of radiation (e.g. Sun or Earth). Contrary to most passive sensors that are bound to detecting either the reflected Sun radiation or emitted radiation by the Earth’s surface in ranges from the ultraviolet to the thermal infrared - active sensors can use any radiation from the electromagnetic spectrum - the only limitation being the transparency of the Earth’s atmosphere. They often use wavelengths that are not sufficiently provided by the Sun - e.g. microwaves. \rActive systems can be categorized either according to their imaging capability - or according to the considered emitted wavelength - or also according to the way they use the returned signal. For the last category - it is generally distinguished between ranging systems - which use as principal information the time delay between transmission and reception of the electromagnetic radiation at the sensor - and scattering systems - which consider the strength (also called magnitude or intensity) - of the returned signal. Some systems also register both information.\rAs active sensors produce their own radiation and do not rely on e.g. Sun radiation - they are daytime independent and can also retrieve information about the Earth’s surface by night. Furthermore - depending of the considered wavelength - active sensors are weather independent. For longer wavelengths of the microwave domain - clouds are transparent - as the transmitted wavelength is larger than the water particles constituting the cloud and do not interact with them. \rActive sensors can control the direction of their illumination to a specific target to be investigated - but require in general more energy than passive sensors as they “actively” illuminate the Earth’s surface.679,PS1-3-2-1-4,Radar altimeters,A radar altimeter is an active - non-imaging remote sensing device. It measures the height of the terrain along the track beneath an air- or spaceborne platform using electromagnetic radiation from the microwave region of the electromagnetic spectrum. Radar altimeters operate similar to laser profilers. Both emit a short pulse of electromagnetic radiation towards the Earth’s surface and detect the time delayed echo. By measuring the time delay and knowing the speed of propagation of the pulse - the range (distance) from the instrument to the surface can be determined. By using the forward motion of the altimeter platform and transmitting a continuous stream of pulses a profile can be built up. If the exact location of the platform as a function of time is known - a surface profile can be generated. \rFor a high accuracy of the range resolution - a narrow antenna beam is required - which can be achieved either by using large antennas or short radar beams. In the first case - the radar altimeter is beam-limited; in the second case it is pulse-limited. As large antennas are not practical in space - pulse-limited systems are used for space-borne platforms. Pulse-limited altimeters use frequency modulated (chirp) pulses generated by a chirp generator. The accuracy of the measurements also depends on atmospheric transmission effects - as the speed of the electromagnetic radiation traveling at the speed of light will be delayed when passing through the ionosphere and the atmosphere twice. In general - the range resolution of radar altimeters is in the order of a few centimetres. \rIn the beginning - radar altimeters were used for measurements of surface profiles of the ocean topography to get information about currents - ocean circulation - wind and waves. Another basic application of altimetry were measurements over ice sheets and glaciers - e.g. for mass balance determination. Further application domains are geoid measurements also revealing deep sea trenches and the precise monitoring of satellite orbits.693,PS1-3-4-2-1,Differential Absorption Lidar,Differential Absorption Lidar (DIAL) is a laser remote sensing technique that is used for range and/or profile measurements of atmospheric gas concentrations and constituents.695,PS1-4,Imaging vs. nonimaging sensors,There are different ways to classify sensors used in remote sensing. One of them is the division into imaging and non-imaging sensors. Imaging sensors typically employ optical imaging systems (from VIS to TIR). They operate primarily at window frequencies - where atmospheric absorption is low and surface features can be imaged or measured. Non-imaging sensors include microwave radiometers - microwave altimeters - magnetic sensors - gravimeters - Fourier spectrometers - laser rangefinders - and laser altimeters.703,PS1,Types of remote sensing sensors,Remote sensing sensors acquire information about objects situated on the surface of e.g. the Earth remotely - e.g. from a distance - without any physical contact. They detect and measure the changes that the object imposes on its. \rRemote Sensing sensors are characterized according to several different properties:\r	Depending on the interaction between the sensor and the Earth’s surface - one distinguishes between active (e.g. radar) and passive (e.g. optical imagery) sensors. Some systems use both kind of sensors simultaneously.\r	Depending on the mapping process of the information - it can be distinguished between imaging and non-imaging sensors. Imaging sensors produce an image of an area of interest - e.g. give a spatial information about the incoming information. Spatial relationships between objects can be identified and used for visual interpretation. Non-imaging sensors register usually single response values for a specific area - and do not record how the incoming information varies across the field of view. They can be used to characterize the interaction between the received information and illuminated target.\r	Depending on the platform on which the instrument is deployed - one speaks either of ground based (e.g. terrestrial laser scanner) - airborne (e.g. plane - drone) - or spaceborne (e.g. satellite) sensor. For spaceborne sensors - the orbit geometry (e.g. geostationary - equatorial - sun-synchronous) and altitude (high - medium and low Earth orbit) play an important role - as it most often determines the application of the satellite in combination with the deployed sensor (weather satellites or Earth observation satellite). \r	Depending on the observed portion of the electromagnetic spectrum (e.g. optical - infrared - thermal - microwave). \r	Depending on the instrument (e.g. imagers - altimeters - spectrometers - radiometers). \r	Depending on the instrument precision - e.g. in terms of spatial resolution very high  vs. low resolution sensors; in terms of spectral resolution narrow band (hyperspectral sensors) vs. broad-band sensors (mono- and multispectral sensors); in terms of radiometric resolution very high vs. low resolution sensors. Some applications do not require very high precision instruments - e.g. sea surface temperature measurements - while other - e.g. for vegetation monitoring - require high spectral and radiometric resolution for good data interpretation and  analysis.   \rOther categorization would include the specific applications of each sensor (weather - environment - urban - land - water - mapping - photogrammetry - structure-from-motion - etc.) and if is financed and used for scientific - commercial or military goals.704,PS2-1,History of Remote Sensing Platforms,This topic covers information on the first remote sensing platforms that were used to obtain aerial photos. The first-known aerial photo was obtained in 1858 by Gaspard Felix Tournachon (Nadar). Afterwards - different platforms were used to obtain the information from above. The history of the development of remote sensing platforms includes platforms such as baloons - kites - rockets - pigeons - gliders - etc. to recent low-cost femtosatellites - e.g. for solar radioation pressure measurements. Historically - the main developments of the platforms as well as sensors was associated with military operations in the XXth century. Remote sensing data was used as part of photo- or/and satellite reconnaissance - i.e. aerial photos or satellite imageries used for the military purposes - mainly to make accurate maps and based on that to prepare a military strategy.706,PS2-2-2-1,Mission planning,Mission planning depends on the selected system of acquisition (sensor and platform). A detailed planning of a mission is a fundamental prerequisite for a successful acquisition of remote sensing data. Planning of an aerial photography mission (manned or unmanned) takes into account several parameters such as time of day/sun angle - weather conditions - flightline - platform. Planning and implementation of a spaceborne Earth Observation mission involves several successive life cycle ‘phases’ of conception - development - production and testing - utilization and support - and retirement - as part of an iterative and recursive process - until the satellite (space segment) is delivered and launched into orbit - and the data are exploited in the ground segment.720,PS2-3-1,Field spectroscopy and portable spectroradiometers,Field spectroscopy generally refers to the use of non-imaging spectrometers near the ground surface and it is usually aimed at evaluating spectral reflectance of the investigated target. For this purpose - consecutive measurements of total incident solar irradiance and of radiance or irradiance upwelling from the target are collected by an operator - or more recently by new instruments for long-term and unattended field spectroscopy measurements. The incident irradiance is usually computed by measuring the radiance upwelling from a white calibrated panel which represents the ideal Lambertian surface. Upwelling fluxes are instead usually collected holding the sensor vertically over the surface (nadir view) - although spectral libraries collected observing the target from different viewing angles are also available. \rField spectrometry is also referred to as ‘proximal sensing’ to underline that spectra are collected with portable spectroradiometers in the vicinity of the target - in contrast to ‘remote sensing’ - which is instead usually performed with satellite or airborne sensors.\rField spectroscopy is therefore an in-situ method for characterising the reflectance of natural or artificial surfaces and thereby provides reference data for the calibration or validation (cal/val) of airborne and satellite sensors. This method provides a means of scaling-up measurements from small areas (e.g. leaves - rocks) to composite scenes (e.g. vegetation canopies) - and ultimately to pixels.\rField spectroscopy is used in different applications - for example - soils - rocks - vegetation and chlorophyll fluorescence - water - snow surfaces and atmosphere. Long-lasting field spectroscopy campaigns based on manual measurements are extremely resource-demanding and do not ensure repeatability of the acquisition conditions as the instrument setup is initialized each day. To overcome such limitations a few research groups have initiated automatic tower-based spectral reflectance measurements using different devices. With such setups - non-imaging spectrometers are installed in the field and are operated automatically for long periods (i.e. months to years) and different networks of hyperspectral instruments are now becoming operational (e.g. RadCal Net).\rField spectroscopy can be also used to predict optimum spectral bands - viewing configuration - spectral calibration and time to perform a particular remote sensing task but also to develop - refine and test models relating biophysical attributes to remotely-sensed data. In this context - ground reflectance measurements are therefore mainly used as input in simulation study for sensor design - calibration/validation data for remote sensing sensors - for spectral mixture analysis and for the development of relationships between field data and radiometric variables.\rSince spectroscopy is the study of matter using electromagnetic radiation -  point or imaging field spectrometers are instruments which allow the measurements of reflected or emitted electromagnetic radiation. In particular - portable or hand-held spectroradiometers are small instruments that spectrally measure the radiation reflected or emitted by a target and they are useful in obtaining accurate spectral data over different surfaces. In remote sensing - they generally cover the 400-2500 nm spectral range and operate with a full width at half-maximum of about 1.5/3 nm - so that they can collect radiation in a continuous way across the spectrum. The final output is therefore the hyperspectral signature of reflectance of the surfaces versus the considered wavelength.722,PS2-3,Ground platforms and systems,Platforms and systems that acquire data from the level of earth's surface. A wide variety of ground based platforms are used in remote sensing. The acquired data are used for detailed in-situ measurements - e.g. - Leaf Area Index (LAI) - and for calibration/validation campaigns.723,PS2,Types of remote sensing platforms and systems,Remote sensing platforms and systems can be static (ground-based platforms) or moving (e.g. airborne or spaceborne platforms - UAVs). A remote sensing platform or system carry a remote sensing sensor. It can operate in near (few centimetres) or far (36 -000 kilometres) altitudes ranges.724,PS3-1,History of remote sensing data carriers,The development of remote sensing data carriers has followed the evolution of the photography - remote sensing sensors and computer platforms. The first remote sensed data was stored using the photography films (e.g. aerial photography - satellite Corona program) - which was later replaced by reel tapes - cartridge - and then removable and hard discs. In the era of big and fast growth of Earth observation data - and technological advancements in digital infrastructure - the satellite data are stored using cloud platforms providing different service models: Infrastructure as a Service - Platform/Software as a Service (e.g.  Copernicus DIAS - Google Earth Engine - open EO). The Cloud offers infrastructure to host - store and process the large amount of data efficiently. For example - the Copernicus Data Information Access Services (DIAS) is a comprehensive cloud-based hosting and processing system for the EO data in particularly for the Sentinels data - the Google’s Earth Engine (GEE) provides access to various satellite and offers processing power with a web-based programming interface - the Amazon Web Services (AWS) has dedicated cloud called ‘Earth on AWS’ - the Microsoft’s cloud called Azure facility the use of AI tools to address environmental challenges. Public solutions - as well as private ones - react with a variety of new and innovative tools - which have been recently developed (e.g. DIAS - ODC - EarthServer - EO Browser - GEE).729,PS3-2,Digital image terminology,Most remote sensing data exist as digital images - and appropriate image processing allows the emphasis of certain aspect and subsequent extraction of information for specific applications.\rA digital image is a representation of the reality as a grid of picture elements. It can be considered as an array of numbers that can be stored and handled by a digital computer. The picture elements are pixels and each pixel has a specific value (usually in grayscale). This value is a digital number (DN) - which usually represents the amount of energy recorded by the sensor at this pixel position or any other characteristic recorded by the sensor - e.g. elevation. \rEach row of the image grid - or matrix - corresponds to one scan line. Each pixel is characterized by its row r and column c position in the image - as well as by its value. Additional geographical information is needed in order to assign a geographic location to a pixel. The digital number are integers usually compressed in one byte (= 8 bit) representation - i.e. each pixel can take 256 values.\rDigital images are raster data - as opposite to vector data. Whereas vector data can be points - lines or polygones - raster data always consist of pixels. A pixel is the smallest element in which an image can be divided into. The pixel size varies depending of the instrument and of the sampling used. Large pixel may contain information about several objects of the recorded scene. However - they only have one value. These are called mixed-pixel - as e.g. several land cover classes are represented within one pixel and they cannot be distinguished from another. \rIn multispectral imagery - each region of the electromagnetic spectrum is recorded in an independent image (band). Therefore - at a specific array position (r -c) - there exist several pixels - each with a specific value corresponding to the energy recorded for the considered band. This result in a three-dimensional matrix. The bands of a multispectral image can be displayed three at a time in the computer using for each band one of the three primary colors red - green and blue (RGB). This is called a color composite image. If the color composite represents a combination of the visible red - green and blue bands in their respective color - the combination is called natural or true color composite - as it corresponds to what the human eye sees naturally. Any other combination - for example considering bands of wavelengths that are not visible for the human eye is called a false color composite. It is often used to highlight the spectral differences and particular image features in order to extract information.736,PS3-4-3,Radiometric resolution,Radiometric resolution can be defined as the ability of an imaging system to record many levels of brightness. Radiometric resolution is defined as the sensitivity of a remote sensing detector to differences in signal strength as it records the radiant flux reflected - emitted - or back-scattered from the terrain. Radiometric resolution refers to the range in brightness levels that can be applied to an individual pixel within an image - determined on a grayscale. E.g. - Sentinel-2 sensor MSI is a 12 bit sensor imaging with 4.096 levels.739,PS3-5-1,Header file,A header file is a seperate file associated with an image file. The header file can be either a plain ASCII-file or a binary file. It contains information about the image file it is associated with. These information can comprise the number of pixels per row (x-direction in a two dimensional image) - also called number of columns - the number of lines or rows (y-direction in a two dimensional image) - the number of bands (corresponding to the z-direction) - pixel spacing and spatial resolution - geographic reference information - the byte order (e.g. big-endian or little-endian) - spectral information for each band - calibration constants and many more. The purpose of a header file is to provide basic information about the properties of the image data either to the user or to a software and enabling a software to correctly load and display the image content. In this way - information contained in a header file can also be called metadata - which is data about the data. The structure and the information contained in a header file of remote sensing imagery can be found in the so-called product information documents. There is also digital imagery used in remote sensing containing the information found in header files not in a separate file but as part of the digital image data itself. In this case this is called header information or a file header - which is usually found at the beginning of the image file. In some cases  - image files may contain several header sections - e.g. theESA Envisat ASAR SAR data imagery contains a Main Product Header and a Specific Product Header section. Header information as part of the image file itself may be stored in ASCII or in binary format - or in a mixed binary format - as it was used for the ESA Envisat SAR data.741,PS3-6,Data formats,Remote Sensing data formats in which the data are organized and stored. The data format for a remote sensing mission is usually chosen based on a number of considerations - including requirements of the sensing system - mission objective - the design and technology of data processing - archiving - and distribution systems - as well as community data standard.752,PS3,Remote sensing data and imagery,Remotely collected data is available from multiple sources and data collection techniques. Data can be obtained from different levels of data acquisition: ground - air or space - as well as using different sensors and wavelengths. Remote sensing data provides the necessary information to help monitor the Earth's surface.753,PS4,Databases of satellite and airborne sensors and missions,The listed databases provide information on past - operational and future remote sensing platforms and sensors. Use the following links to get more information on the sensors and missions.762,TA11-2-2,Users in oil & gas,The EO/GI user community in oil & gas consists of offshore exploration and production - on-shore exploration and production - drilling and support services - oil and gas commodities trading - and energy planners. Due to their activities both on-shore and offshore their need for EO-derived information about the land - the ocean and the atmosphere. They need EO-derived information about geological features (for exploration) - for asset infrastructure monitoring - construction and buildings. Safe offshore operations (ocean&atmosphere: forecast and monitoring current movement and drift - monitor sea-ice and icebergs - detect and monitor hurricanes and typhoons; land: map and assess flooding - detect wildfires . A large set of information needs results from their need to adhere to environmental regulations. They have to assess and monitor their environmental impact - ocean quality and productivity - land ecosystems and biodiversity - groundwater and run-off \rMany problems faced by oil - gas - including the selection and development of exploration areas - detection and mapping of illegal mining activities - or monitoring dams - pipelines and terrain movements - can be efficiently addressed by extracting information from geospatial imagery. Remote Sensing based applications reduce the need for field work - minimize environmental impacts - and ultimately safe costs - to help achieve results faster during exploration - extraction - and remediation/reclamation stages.790,TA11-8-3,Users in education - training & research,The community of users in education includes instructors (1) who are teaching or conducting research in some aspect of GIScience - such as coding - remote sensing - field methods - geodetic control - web mapping - spatial analysis - or related topics - or (2) who are using GIS as a teaching tool in a discipline - such as business - biology - economics - or health sciences.  By extension - this community includes students and supportive deans and other educational administrators.  The benefits that these users gain from EO information includes a set of best practices vetted by experts in the field that they can use to teach modern GIS workflows more effectively.  \rThe goals of this user community are focused on a deeper and a broader implementation of geotechnology - methods - and spatial data throughout the educational system—primary - secondary - university - and lifelong learning (libraries - museums - and other informal settings).   Deeper implementation implies embracing GIS as a platform - including its field data gathering tools and citizen science workflows - spatial analysis - building web maps and apps - communicating with multimedia maps derived from web GIS - systems configuration work - and the coding that is behind modern GIS infrastructure.   Broader implementation implies the use of GIS in a multitude of disciplines at all levels of education - formal and informal; occurring wherever changes over space and time are being examined.  \rAt all levels of education the challenge of sufficient bandwidth and the use of a professional systems-based tool such as GIS - along with devices capable of running web GIS tools - are barriers in many areas throughout the world.  However - educational and societal forces represent a stronger challenge than technological ones.  These educational and societal challenges that this user community faces include the lack of educational content standards at the primary and secondary level that support the use of geotechnologies in education - and at the university level - a lack of awareness of and access to modern SaaS GIS tools and open data portals.   \rThe risks that the community faces in not facing the challenge of the use of GIS in the education sector is a lack of geographic and spatial literacy among students and faculty.  This will translate to research that does not consider spatiotemporal implications of 21st Century challenges - a workforce ill-equipped to deal with them - and consequently an increasingly unstable and dysfunctional world.  To build a workforce that can meet global challenges in energy - biodiversity - climate - natural resources - natural hazards - human health - economic inequality - and others - a deep and wide implementation of GIS technology and methods must take place throughout the educational system.  The actions that society can take to face that challenge is to provide professional development opportunities for faculty - curricular resources - assessment instruments - relevant spatial data and open data portals - examples of best practices - and a network for educators and researchers in which to interact.  EO can provide all of these elements in partnership with educational institutions - government - nonprofits - and industry to meet this challenge.  In so doing - an increasingly sustainable - healthier - resilient world can be achieved from the community to the global level.806,TA13-1-3,Forecast the weather,The weather is the state of the atmosphere measurable by its temperature - humidity - precipitation - and other atmospheric variables. To forecast the weather is a major branch in the field of meteorology. In comparison to climate - weather can only be predicted for a short period of time (minutes to month) - because it describes the state of the atmosphere for specific days at specific locations. For a reliable weather forecast - a good numerical prediction model with precise initial conditions is needed. Models are sensitive to changes in the initial condition - that is why at the moment weather predictions are only accurate for few days. However - both models and the determination of initial conditions are steadily improved. EO makes a significant contribution to improving the initial conditions by providing global information several times a day. As the quality of the EO products improves - the weather forecast also improves. \rSince decades - satellites are used to monitor and forecast weather. Therefore - it is one of the most established sectors of satellite data applications. There are geostationary and polar-orbiting weather satellites that measure all kinds of meteorologically relevant variables - e.g. cloud coverage - wind speed [...] via passive or active imagery. However - not only satellites are used to collect information - but also other remote sensing techniques that can be airborne or ground-based such as Lidar.\rWeather forecasts are used by citizens for decisions in everyday life - in agriculture for crop cultivation decisions and in the stock markets. Other domains of applications are hydrometeorology - aviation - maritime navigation - and the military and nuclear sectors.814,TA13-3-2,Detect and monitor wildfires,For the outbreak of forest fires - satellite remote sensing can be continuously track and monitor - in a timely manner to grasp the development of forest fires. Beyond - weather monitoring enables to forecast weather conditions where fires are likely - allowing authorities to prepare.816,TA13-3-4,Forecast and assess landslides,Landslides are a natural hazard posing a threat to human life - property - infrastructure - and natural environment. Every year - slope instabilities have a significant impact on societies and economies. Consequently - landslide documentation is used for risk assessments - policy making and enforcing of construction regulations. Landslide monitoring is used to ensure safety of infrastructure operation. Rapid mapping of landslides and associated damages is done for response actions - e.g. of civil protection organizations. As ground surveys are very costly and time-consuming - satellite remote sensing is increasingly used to assess damage resulting from landslides.\rLandslides lead to local terrain changes after a downslope movement of material under the effect of gravity. They vary by type of movement (e.g. falling - toppling - gliding and flowing) - by size (from small rocks to entire mountain slopes) and velocity (from a couple of millimetres per year up to free-fall speed). Landslides can be triggered both by natural causes (like earthquakes or heavy rainfall events) and human causes - e.g. mining activities that lead to slope failures. Landslides can initiate other natural hazards - e.g. when a landslide blocks a river a lake can be formed which poses a risk for an outburst flood. \rLandslides are diverse in appearance - and therefore are challenging to detect. EO-based assessment methods aim for detecting changes to the land surface and surface displacements. \rEO satellites and airborne remote sensing use optical sensors for detecting landslides in post-event images and land cover changes caused by landslides - primarily indicated by the removal of vegetation and the exposure of bare soil - by comparing pre-event and post-event images. Typical resolutions of optical EO data for mapping rapid landslides are between 0.4 m and 30 m - depending on the size of landslides caused by the triggering event. Optical data from unmanned aerial vehicles are used in cases where single landslides or concise regions have to be covered. Additionally - synthetic aperture radar (SAR) sensors allow the detection of subtle changes in ground deformation caused by landslides. Therefore - time-series of radar images are used. Further - airborne laser scanning enables the generation of digital elevation models (DEMs) that allow identification of landslide surface structures and - in case of repeated coverage - detection of elevation changes. DEM generation for analysing landslides is also possible with photogrammetry on stereographic optical data and radargrammetry on SAR images.\rThe diversity of appearances of landslides leads to challenges for (semi-)automatic image processing and makes visual interpretation of EO data by a landslide expert a commonly used method for landslide mapping. However - visual interpretation is subjective and experts’ results can be very diverse. Additionally - it is a slow and time-consuming process. Semi-automated classification based on optical and DEM data using object-based image analysis (OBIA) can achieve detailed interpretations of landslides while reducing the analysis time. Interferometic SAR (InSAR) techniques - such as persistant scatterer interferometry (PSI) or Small Baseline Subset (SBAS) - are primarily used to identify and monitor slow-moving landslides and for quantifying movement rates. Integrated analysis of optical - DEM and SAR data allow to fully exploit the potential of EO data from different sensors for landslide mapping and assessment.835,TA13-6-4,Monitor fisheries,To support an ecosystem-based approach for fisheries management - EO images with global and daily systematic coverage with high-resolution images can help in identifying potential fishing zones and to assess fish stocks. They help assessing and understanding changing abundancy and spatial distribution of exploited fish stocks. Therefore - they analyse various key environmental parameters that can be detected with satellite remote sensing. This includes sea surface temperatures (SSTs) - sea surface height anomalies - and sea surface colour revealing the abundance of chlorophyll a. This relates to phytoplacton production that is directly related to total fish landings. Additionally - EO can detect harmful algal bloom. A further threat to sustainable fish stocks management are illegal fishing. Where localization of licensed fishing vessels and fleet management services are supported by EO to avoid overexplotation and enable recovery of fish stocks. EO complements identification - detection and tracking of vessels with SAR and optical remote sensing.836,TA13-6-5,Detect and monitor ships,For shipping - navigation - and monitoring sea-traffic and pollution - remote sensing and satellite technologies allow detecting vessels in the wider ocean. EO can detect the vessels themselves - their wake trailing behind them - sandbanks and reefs that pose a threat for safe navigation. Additionally - EO can detect pollution from the ships - e.g. when illegal waste disposal happens. Ship detection and classification is possible with the use of optical and synthetic aperture radar (SAR) imagery. The methods complement each other.